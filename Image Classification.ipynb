{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils as utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LOAD DATA**\n",
    "\n",
    "I created a dataset class to load the images from local directories and used Dataloader to load the training and testing set. Note here that to align with the structure of the dataset classes, I used ```product id``` of each testing image as the ```label```.\n",
    "\n",
    "Note that the dataset ```5_shot``` is in the ```PATH``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY = range(22)\n",
    "# PATH = '/Users/zhilinzhou/Desktop/'\n",
    "TRAIN_PATH = PATH + '5_shot/train/'\n",
    "TEST_PATH = PATH + '5_shot/test'\n",
    "transform_pipe = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(size=(224, 224)),  \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, dir, train=False, categories=None, transform=None):\n",
    "        self.dir, self.labels = [], []\n",
    "        self.size = 0\n",
    "        if train:\n",
    "            for c in categories:\n",
    "                path = dir + str(c)\n",
    "                os.chdir(path)\n",
    "                for file in os.listdir():\n",
    "                    if file.endswith('.jpg'):\n",
    "                        self.dir.append(f\"{path}/{file}\")\n",
    "                        self.labels.append(c)\n",
    "                        self.size += 1\n",
    "        else:\n",
    "            os.chdir(dir)\n",
    "            for file in os.listdir():\n",
    "                if file.endswith('.jpg'):\n",
    "                    self.dir.append(f\"{dir}/{file}\")\n",
    "                    self.labels.append(file[:-4])\n",
    "                    self.size += 1\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.dir[idx])\n",
    "        image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 training data, 517 test data.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = myDataset(dir=TRAIN_PATH, train=True, categories=CATEGORY, transform=transform_pipe)\n",
    "TEST_DATA = myDataset(dir=TEST_PATH, transform=transform_pipe)\n",
    "TRAIN_LOADER = utils.data.DataLoader(\n",
    "    TRAIN_DATA,\n",
    "    batch_size=8\n",
    ")\n",
    "TEST_LOADER = utils.data.DataLoader(\n",
    "    TEST_DATA,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(f\"{TRAIN_DATA.size} training data, {TEST_DATA.size} test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = torchvision.models.resnet50(pretrained=True)\n",
    "MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myModel():\n",
    "    def __init__(self, model, train_loader, test_loader, device, optimizer, num_epoch=15):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.num_epoch = num_epoch\n",
    "        ## optimizer?\n",
    "        if optimizer == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=2e-4)\n",
    "        elif optimizer == 'AdamW':\n",
    "            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-4, weight_decay=0.1)\n",
    "        else:\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.128, momentum=0.875, weight_decay=3)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def train(self):\n",
    "        print(\"Start training...\")\n",
    "        self.model.train()\n",
    "        total_loss, total_accuracy = [], []\n",
    "        n = len(self.train_loader.dataset)\n",
    "        for epoch in range(1,self.num_epoch+1):\n",
    "            loss_, accuracy_ = 0, 0\n",
    "            samples = 0\n",
    "            for i, batch in enumerate(self.train_loader, 0):\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                result = self.model(inputs)\n",
    "                loss = self.criterion(result, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                loss_ += loss.item()\n",
    "                correct = np.sum(np.argmax(result.detach().numpy(), axis=1) == labels.detach().numpy())\n",
    "                accuracy_ += correct\n",
    "                samples += inputs.shape[0]\n",
    "\n",
    "                if i % 10 == 0:\n",
    "                    print(\"Epoch: %d\\t [%d/%d - %d%%]: loss - %.4f \\t accuracy - %.2f%%\" \\\n",
    "                            % (epoch, samples, n, samples/n*100, loss/samples, accuracy_/samples*100))\n",
    "            total_loss.append(loss_ / samples)\n",
    "            total_accuracy.append(accuracy_ / samples)\n",
    "            print(\"==> Epoch %d: loss - %.4f, accuracy - %.2f%%\" \\\n",
    "                    % (epoch, total_loss[-1], total_accuracy[-1]*100))\n",
    "        print(\"Training finished!\\n\")\n",
    "        return total_loss, total_accuracy\n",
    "    \n",
    "    def test(self):\n",
    "        print(\"Start testing...\")\n",
    "        self.model.eval()\n",
    "        ids = []\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(self.test_loader, 0):\n",
    "                inputs, labels = batch\n",
    "                inputs = inputs.to(device)\n",
    "                for id_ in labels:\n",
    "                    ids.append(id_)\n",
    "                pred = self.model(inputs)\n",
    "                predictions.append(np.argmax(pred.detach().numpy(), axis=1))\n",
    "        print(\"Testing finished!\\n\")\n",
    "        return ids, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRESENT RESULTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch: 1\t [8/110 - 7%]: loss - 1.0844 \t accuracy - 0.00%\n",
      "Epoch: 1\t [88/110 - 80%]: loss - 0.0853 \t accuracy - 0.00%\n",
      "==> Epoch 1: loss - 0.9977, accuracy - 0.00%\n",
      "Epoch: 2\t [8/110 - 7%]: loss - 0.1865 \t accuracy - 87.50%\n",
      "Epoch: 2\t [88/110 - 80%]: loss - 0.0279 \t accuracy - 54.55%\n",
      "==> Epoch 2: loss - 0.3214, accuracy - 45.45%\n",
      "Epoch: 3\t [8/110 - 7%]: loss - 0.0165 \t accuracy - 100.00%\n",
      "Epoch: 3\t [88/110 - 80%]: loss - 0.0064 \t accuracy - 92.05%\n",
      "==> Epoch 3: loss - 0.0964, accuracy - 89.09%\n",
      "Epoch: 4\t [8/110 - 7%]: loss - 0.0217 \t accuracy - 100.00%\n",
      "Epoch: 4\t [88/110 - 80%]: loss - 0.0038 \t accuracy - 96.59%\n",
      "==> Epoch 4: loss - 0.0356, accuracy - 97.27%\n",
      "Epoch: 5\t [8/110 - 7%]: loss - 0.0106 \t accuracy - 100.00%\n",
      "Epoch: 5\t [88/110 - 80%]: loss - 0.0015 \t accuracy - 100.00%\n",
      "==> Epoch 5: loss - 0.0157, accuracy - 100.00%\n",
      "Epoch: 6\t [8/110 - 7%]: loss - 0.0046 \t accuracy - 100.00%\n",
      "Epoch: 6\t [88/110 - 80%]: loss - 0.0004 \t accuracy - 100.00%\n",
      "==> Epoch 6: loss - 0.0069, accuracy - 100.00%\n",
      "Epoch: 7\t [8/110 - 7%]: loss - 0.0028 \t accuracy - 100.00%\n",
      "Epoch: 7\t [88/110 - 80%]: loss - 0.0002 \t accuracy - 100.00%\n",
      "==> Epoch 7: loss - 0.0036, accuracy - 100.00%\n",
      "Epoch: 8\t [8/110 - 7%]: loss - 0.0021 \t accuracy - 100.00%\n",
      "Epoch: 8\t [88/110 - 80%]: loss - 0.0002 \t accuracy - 100.00%\n",
      "==> Epoch 8: loss - 0.0026, accuracy - 100.00%\n",
      "Epoch: 9\t [8/110 - 7%]: loss - 0.0017 \t accuracy - 100.00%\n",
      "Epoch: 9\t [88/110 - 80%]: loss - 0.0002 \t accuracy - 100.00%\n",
      "==> Epoch 9: loss - 0.0021, accuracy - 100.00%\n",
      "Epoch: 10\t [8/110 - 7%]: loss - 0.0015 \t accuracy - 100.00%\n",
      "Epoch: 10\t [88/110 - 80%]: loss - 0.0001 \t accuracy - 100.00%\n",
      "==> Epoch 10: loss - 0.0018, accuracy - 100.00%\n",
      "Epoch: 11\t [8/110 - 7%]: loss - 0.0013 \t accuracy - 100.00%\n",
      "Epoch: 11\t [88/110 - 80%]: loss - 0.0001 \t accuracy - 100.00%\n",
      "==> Epoch 11: loss - 0.0015, accuracy - 100.00%\n",
      "Epoch: 12\t [8/110 - 7%]: loss - 0.0012 \t accuracy - 100.00%\n",
      "Epoch: 12\t [88/110 - 80%]: loss - 0.0001 \t accuracy - 100.00%\n",
      "==> Epoch 12: loss - 0.0014, accuracy - 100.00%\n",
      "Epoch: 13\t [8/110 - 7%]: loss - 0.0010 \t accuracy - 100.00%\n",
      "Epoch: 13\t [88/110 - 80%]: loss - 0.0001 \t accuracy - 100.00%\n",
      "==> Epoch 13: loss - 0.0012, accuracy - 100.00%\n",
      "Epoch: 14\t [8/110 - 7%]: loss - 0.0009 \t accuracy - 100.00%\n",
      "Epoch: 14\t [88/110 - 80%]: loss - 0.0001 \t accuracy - 100.00%\n",
      "==> Epoch 14: loss - 0.0011, accuracy - 100.00%\n",
      "Epoch: 15\t [8/110 - 7%]: loss - 0.0009 \t accuracy - 100.00%\n",
      "Epoch: 15\t [88/110 - 80%]: loss - 0.0001 \t accuracy - 100.00%\n",
      "==> Epoch 15: loss - 0.0010, accuracy - 100.00%\n",
      "Epoch: 16\t [8/110 - 7%]: loss - 0.0008 \t accuracy - 100.00%\n",
      "Epoch: 16\t [88/110 - 80%]: loss - 0.0001 \t accuracy - 100.00%\n",
      "==> Epoch 16: loss - 0.0010, accuracy - 100.00%\n",
      "Epoch: 17\t [8/110 - 7%]: loss - 0.0007 \t accuracy - 100.00%\n",
      "Epoch: 17\t [88/110 - 80%]: loss - 0.0001 \t accuracy - 100.00%\n",
      "==> Epoch 17: loss - 0.0009, accuracy - 100.00%\n",
      "Epoch: 18\t [8/110 - 7%]: loss - 0.0007 \t accuracy - 100.00%\n",
      "Epoch: 18\t [88/110 - 80%]: loss - 0.0001 \t accuracy - 100.00%\n",
      "==> Epoch 18: loss - 0.0008, accuracy - 100.00%\n",
      "Epoch: 19\t [8/110 - 7%]: loss - 0.0006 \t accuracy - 100.00%\n",
      "Epoch: 19\t [88/110 - 80%]: loss - 0.0001 \t accuracy - 100.00%\n",
      "==> Epoch 19: loss - 0.0008, accuracy - 100.00%\n",
      "Epoch: 20\t [8/110 - 7%]: loss - 0.0006 \t accuracy - 100.00%\n",
      "Epoch: 20\t [88/110 - 80%]: loss - 0.0001 \t accuracy - 100.00%\n",
      "==> Epoch 20: loss - 0.0007, accuracy - 100.00%\n",
      "Training finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run on GPU if possible\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "model = myModel(MODEL, TRAIN_LOADER, TEST_LOADER, device, 'AdamW', num_epoch=20)\n",
    "loss, accuracy = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **OUTPUT PREDICTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n",
      "Testing finished!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category\n",
       "id           \n",
       "63          3\n",
       "189         7\n",
       "77          3\n",
       "162         6\n",
       "176         7\n",
       "88          3\n",
       "348        17\n",
       "360        15\n",
       "406        17\n",
       "412        17"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id, predictions = model.test()\n",
    "submissions = pd.DataFrame({\n",
    "    'id': id,\n",
    "    'category': np.concatenate(predictions).reshape(-1,).astype(\"int\")\n",
    "}).set_index('id')\n",
    "submissions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PATH)\n",
    "submissions.to_csv(\"final_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
